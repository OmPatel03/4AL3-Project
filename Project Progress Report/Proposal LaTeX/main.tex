\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{float} % 

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{Group 51 Progress Report:\\The Optimal Match Model: Predicting Ideal Partner with ML}


\author{Alvin Qian , Om Patel, Gregory Archer\\
  \texttt{\{qiana2,patelo11,archeg1\}@mcmaster.ca} }


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

Online dating platforms often optimize for engagement metrics such as clicks, swipes, or stated preferences rather than genuine mutual compatibility. Many systems therefore prioritize popularity or activity rather than understanding the bidirectional nature of attraction. In this project, we study the problem of predicting an individual's ideal partner profile from structured demographic, preference, and rating data. Our goal is not only to identify who a person is likely to say ``yes'' to but also to infer what kind of person would, in turn, find them compatible.

Given a participant A, our model predicts the set of individuals A would likely respond positively to, based on their recorded preferences and historical choices. From the top-$k$ candidates, we generate a composite partner profile $B^*$ representing the aggregated traits of A’s most desired matches. The model then predicts who $B^*$ would be most likely to say ``yes'' to, forming another representative profile $C^*$. Comparing A and $C^*$, referred to as the preference gap, highlights asymmetries between what an individual seeks and what those they desire tend to prefer. This framing offers insight into unreciprocated attraction and the dynamics of compatibility, while providing both recommendation value and interpretability.

The task is formulated as a binary classification problem (predicting the ``yes'' or ``no'' decision) with ranking-based evaluation to measure recommendation quality. Our objectives are threefold:
\begin{enumerate}
    \item Build a leakage-free, explainable pipeline that predicts an individual’s decision outcome.
    \item Construct the composite profiles $B^*$ and $C^*$ for analyzing preference gaps.
    \item Evaluate performance using metrics such as Accuracy, F1 score, ROC-AUC, and Precision@k.
\end{enumerate}




\section{Related Work}

Our work is informed by several research areas. The first is the original analysis of our dataset, the Columbia Speed Dating Experiment, where Fisman and Iyengar found that attractiveness, fun, and shared interests were highly predictive of matching decisions \citep{Fisman:2006}. Second, our A→B→C loop concept is inspired by two-sided matching theory \citep{Roth:online}, most famously represented by the Gale–Shapley algorithm \citep{Gale:1962}, which underscores that stable matches must account for the preferences of both sides. 

Third, our approach is related to the broader field of recommender systems, which often use collaborative or content-based filtering to predict preferences, though typically not in a two-hop ``preference gap'' framework; our evaluation strategy aligns with modern recommender system assessment practices \citep{Li:2020}. In particular, our use of ranking-based metrics such as Precision@k parallels the long-standing information retrieval literature on top-$k$ evaluation, where early foundational work by Järvelin and Kekäläinen established gain-based ranking metrics and motivated precision-oriented top-$k$ assessment \citep{jarvelin2002ir}.

Fourth, we draw from psychological research on partner preferences, such as the meta-analysis by \citet{Eastwick:2014}, which reviewed the predictive validity of stated ideal partner preferences. Finally, given our goal of providing insights, our work connects to explainable AI (XAI) methods, such as SHAP \citep{Lundberg:2017}, which we plan to use to identify which features most influence compatibility predictions.

\section{Dataset}

We are using the Columbia Speed Dating dataset, as described in our proposal. It consists of \textbf{8,378 observations} (individual dates) and \textbf{123 columns}.
\subsection{Preprocessing and Cleaning}

The raw dataset required significant preprocessing. Our pipeline performs the following steps:
\begin{itemize}
    \item \textbf{String Decoding:} Many text columns were encoded as Python byte literals (e.g., `b'female'`). We decoded these into standard UTF-8 strings.
    \item \textbf{Normalization:} All string values were converted to lowercase and stripped of leading/trailing whitespace to ensure consistency (e.g., "Law" and "law" are treated as identical).
    \item \textbf{Missing Values:} We unified various missing value markers (e.g., "na", "n/a", "", "nan") into a single `pd.NA` representation.
    \item \textbf{Numeric Coercion:} Columns that appeared to be numeric but were stored as objects (e.g., "1.0", "5") were automatically coerced into floating-point types, while preserving categorical ranges (e.g., "[0-1]").
\end{itemize}

\section{Features}

Our model inputs were derived entirely from pre-event survey responses in the Speed Dating dataset to avoid post-event leakage and ensure that predictions reflected true compatibility rather than impressions formed during the event. Each training example represents a pairing between two participants (A and B), and the model receives features describing both individuals. This allows the classifier to learn relationships between what A wants in a partner and how B presents themselves, as well as broader patterns of interpersonal compatibility.

\subsection{Base Feature Set}

For participant A, we included demographic attributes (age, gender, race, field of study), self-assessment ratings (attractive, sincere, intelligent, funny, ambitious), personal interests (e.g., sports, music, movies, reading, exercise, hiking, art, shopping), and stated partner preferences (such as \texttt{attractive\_important} and \texttt{sincere\_important}). For participant B, we incorporated the corresponding partner-side features: demographic attributes (age\_o, gender\_o, race\_o), self-ratings (\texttt{attractive\_o}, \texttt{sincere\_o}, \texttt{intelligence\_o}, etc.), and B's stated preferences for a partner (\texttt{pref\_o\_attractive}, \texttt{pref\_o\_sincere}, etc.). This symmetry ensures that the model observes both what A is seeking and what B offers.

Categorical variables were decoded from byte strings and then one-hot encoded. Numeric values were coerced into consistent formats, but no scaling was required since tree-based models are insensitive to feature magnitude.

\subsection{Interaction and Augmented Features}

In addition to the base features, we experimented with an augmented ``rich'' feature set that incorporated all non-leaky \texttt{d\_*} interaction columns present in the dataset. These columns encode differences between A and B along various traits, such as \texttt{d\_age}, \texttt{d\_attractive}, \texttt{d\_funny}, and \texttt{d\_music}. Including these relative-difference features is motivated by the literature on mate selection, which emphasizes alignment and distance between partners rather than absolute traits alone. Our ablation results confirmed this intuition: the model performed substantially better when interaction features were included.

\subsection{Rationale}

It is natural to include both absolute features (e.g., age, interests, personality) and relative features (e.g., age gap, rating differences), since attraction depends simultaneously on individual attributes and their compatibility. Partner preference variables (\texttt{\_important} and \texttt{pref\_o\_}) were included because prior psychological studies show that stated preferences predict real choices to a moderate degree. Interest-based features were included because similarity in hobbies has been shown to influence perceived compatibility.

\subsection{Feature Experiments}

We compared two main feature configurations:

\begin{enumerate}
    \item \textbf{Base Pre-Event Features}—demographics, interests, self-ratings, and preference indicators.
    \item \textbf{Augmented Features with Interaction Deltas}—the base set plus all non-leaky \texttt{d\_*} difference features.
\end{enumerate}

This variation allowed us to directly measure the contribution of relational features. Models trained on the rich feature set consistently outperformed those trained on the base set, indicating that the differences between A and B were critical for accurate prediction.

No dimensionality reduction or learned embeddings were used, as the feature dimensionality was manageable and tree-based models naturally handle large sparse inputs. Future work may explore representation learning or feature pruning informed by model-based importance scores (e.g., SHAP values) to improve interpretability.


\section{Implementation}

We implemented a series of binary classification models to predict whether a participant (A) would say ``yes'' to another participant (B) using only pre-event survey features from the Speed Dating dataset. The prediction target is the \texttt{decision} label, where 1 indicates a positive response and 0 a negative one. All modeling decisions were made to prevent post-event leakage and ensure fair, reproducible evaluation. Although the broader goal of the project includes the A~$\rightarrow$~B~$\rightarrow$~C pipeline for analyzing multi-stage compatibility patterns, the implementation described here focuses on building the core predictive model that underlies that pipeline.

\subsection{Baselines}

Our simplest baseline is a majority-class classifier that always predicts ``no.'' Since the dataset is imbalanced (approximately 58\% negative and 42\% positive decisions), this baseline achieves 58\% accuracy but an F1 score of 0. This provides a conservative lower bound and highlights the necessity of learning meaningful structure beyond class priors.

As a stronger baseline, we trained a logistic regression model with one-hot encoded categorical variables. This model achieved modest improvements (accuracy $\approx$ 0.60, ROC-AUC $\approx$ 0.62), demonstrating that linear models capture some predictive structure but are insufficient for modeling the complex, nonlinear preference patterns in the dataset.

\subsection{Primary Models}

Our primary approach uses gradient-boosted decision trees. We experimented with two variants:

\begin{itemize}
    \item \textbf{HistGradientBoostingClassifier (HGB)}, a fast, histogram-based gradient-boosting model suitable for tabular data.
    \item \textbf{XGBoost}, a state-of-the-art gradient-boosting framework offering strong regularization and flexible tree construction.
\end{itemize}

Both models were trained with the binary cross-entropy (log loss) objective, optimized via additive boosting. Hyperparameters such as \texttt{max\_depth}, \texttt{learning\_rate}, \texttt{subsample}, and \texttt{n\_estimators} were tuned through randomized search. Early stopping on a held-out validation set prevented overfitting and greatly reduced training time.

Categorical variables were encoded using a \texttt{ColumnTransformer} with one-hot encoding, while numeric features were passed through unchanged. Since probability quality is important for downstream analysis, we applied isotonic regression to calibrate model outputs, ensuring that predicted probabilities accurately reflected empirical decision frequencies.

\subsection{Feature Variants and Ablations}

We evaluated two feature variants:

\begin{enumerate}
    \item \textbf{Base Feature Set:} demographics, self-ratings, preferences, and interests available from pre-event surveys.
    \item \textbf{Rich Feature Set with Interaction Deltas:} an augmented feature set including all non-leaky interaction columns of the form \texttt{d\_*}, which quantify differences between A and B (e.g., age difference, rating gaps, interest mismatch).
\end{enumerate}

These ablations allowed us to isolate the contribution of interaction features. The interaction-enhanced model performed substantially better, indicating that relational differences between A and B are highly predictive of attraction decisions. These findings are consistent with prior literature on partner preference alignment.

\subsection{Optimization Strategy}

Participants were partitioned at the individual level to avoid having the same person appear in multiple splits. Data were divided into 70\% training, 15\% validation, and 15\% testing. All optimization followed XGBoost's gradient-boosting procedure, with subsampling-based regularization and early stopping to prevent overfitting. Model selection was based on validation ROC-AUC.

\subsection{Results}

Using the base feature set, our calibrated XGBoost model achieved accuracy $\approx$ 0.63, F1 $\approx$ 0.51, and ROC-AUC $\approx$ 0.66, outperforming both baselines. Incorporating interaction deltas led to a substantial improvement: the interaction-enhanced XGBoost model achieved accuracy $\approx$ 0.73, F1 $\approx$ 0.70, and ROC-AUC $\approx$ 0.80. These results demonstrate that gradient-boosted tree models effectively capture pre-event compatibility and provide a strong foundation for our planned A~$\rightarrow$~B~$\rightarrow$~C compatibility pipeline.

While we implemented an initial version of the pipeline earlier in the project using a simpler model, the primary focus of this report is the construction and evaluation of the final predictive models themselves. The improved probability calibration and richer feature representations documented here position the model to support more reliable downstream analyses—including A’s top predicted partners (B), the composite “ideal partner” profile (B$^\ast$), and B$^\ast$’s preferred matches (C)—in future extensions of the project.


\section{Results and Evaluation}

We evaluated our models using a participant-level split of 70\% training, 15\% validation, and 15\% testing, ensuring that no individual appeared in multiple subsets. This prevents information leakage across splits and provides a fair basis for model comparison. All models were trained only on pre-event (pre-date) features, and probability outputs were calibrated using isotonic regression on the validation set.

\subsection{Baselines}

As a sanity check, we first considered a majority-class baseline that always predicts ``no.'' Because roughly 58\% of decisions in the dataset are negative, this baseline achieves 0.58 accuracy but an F1 score of 0, since it never predicts the positive class. This confirms that any useful system must go beyond class priors.

We then trained a \textbf{clean product-safe} model using a \texttt{HistGradientBoostingClassifier} (HGB) on the hand-selected feature set (\texttt{SAFE\_FEATURES}) combining A's demographics, self-ratings, interests, and partner preferences, plus a small set of interaction terms (\texttt{samerace}, \texttt{d\_age}, \texttt{interests\_correlate}, \texttt{importance\_same\_race}, \texttt{importance\_same\_religion}). This model achieved:
\begin{itemize}
    \item \textbf{Accuracy:} 0.6152
    \item \textbf{Precision:} 0.5185
    \item \textbf{Recall:} 0.4902
    \item \textbf{F1 Score:} 0.5039
    \item \textbf{ROC-AUC:} 0.6435
\end{itemize}
These results show that even a relatively simple tree-based model can capture non-trivial structure in the data, but its discriminative power remains limited.

\subsection{Base XGBoost Model}

Next, we trained an XGBoost-based A$\rightarrow$B model on the same clean feature set. Using the same participant-level split and isotonic calibration, the base XGBoost classifier achieved:
\begin{itemize}
    \item \textbf{Accuracy:} 0.6326
    \item \textbf{Precision:} 0.5455
    \item \textbf{Recall:} 0.4726
    \item \textbf{F1 Score:} 0.5064
    \item \textbf{ROC-AUC:} 0.6561
\end{itemize}
Compared to HGB, this model provides a modest gain in accuracy and ROC-AUC, but its F1 score remains similar. This suggests that, given only the base feature set, model choice alone yields limited improvements: the main bottleneck is the information content of the features, not the capacity of the classifier.

\subsection{Interaction-Enhanced (Rich) XGBoost Model}

To more directly model compatibility, we extended the feature space with a rich set of interaction deltas, including all non-leaky \texttt{d\_*} columns encoding pairwise differences in self-ratings, interests, and preferences. In total, we found 53 such interaction features, which were added on top of the original A and B attributes.

The interaction-enhanced XGBoost model achieved the following metrics on the held-out test set:
\begin{itemize}
    \item \textbf{Accuracy:} 0.7286
    \item \textbf{Precision:} 0.6272
    \item \textbf{Recall:} 0.7877
    \item \textbf{F1 Score:} 0.6984
    \item \textbf{ROC-AUC:} 0.7990
\end{itemize}
This represents a substantial improvement over both the HGB baseline and the base XGBoost model. Accuracy increases by more than 0.09 absolute compared to the clean HGB model, and the F1 score rises from roughly 0.50 to 0.70. The high recall (0.7877) indicates that the model successfully recovers most of the positive decisions (successful dates), while the ROC-AUC close to 0.80 shows strong separability between compatible and incompatible pairs.

\subsection{Ranking Performance (Precision@k)}

While classification metrics evaluate a model's ability to predict individual
decisions, real recommender systems must also produce high-quality \emph{ranked}
suggestions. To assess this, we computed Precision@k for several values of $k$,
measuring the proportion of true positive matches contained within each
participant's top-$k$ recommended partners.

Using the interaction-enhanced XGBoost model, we obtained:
\begin{itemize}
    \item \textbf{Precision@1:} 0.5926
    \item \textbf{Precision@3:} 0.5926
    \item \textbf{Precision@5:} 0.5481
    \item \textbf{Precision@10:} 0.4511
\end{itemize}
These results show that nearly 60\% of participants most likely predicted match
(top-1) corresponds to an actual positive decision in the dataset. Precision
remains strong at $k=3$ and gradually declines as the recommendation list
expands, which is expected behavior for ranking metrics. Even at $k=10$, almost
half of the returned candidates were true matches, indicating that the model is
able to meaningfully prioritize compatible partners far above chance levels.
\section{Error Analysis}
\label{sec:error-analysis}

To better understand the behavior of our models beyond aggregate metrics, we performed a detailed error analysis on the held-out test set. We focus on three aspects: (1) how errors are distributed across classes and decision thresholds, (2) which feature patterns are associated with systematic mistakes, and (3) how error patterns change between the base feature set and the interaction-enhanced model.

\subsection{Global Error Patterns}

Using a fixed threshold of 0.5 on calibrated probabilities, the interaction-enhanced XGBoost model achieves substantially higher accuracy and F1 score than both the majority baseline and the calibrated XGBoost model trained only on the base pre-event features. However, the confusion matrix reveals that errors are not uniformly distributed.

First, the model is conservative relative to the true label distribution. It correctly identifies most negative decisions, but the false negative rate remains higher than ideal, showing the fact that many successful dates correspond to situations where pre-event attributes appear only moderately favorable. This is consistent with the class imbalance in the dataset and the decision to optimize for overall F1 score instead of recall alone.

Figure~\ref{fig:true-false-positives} shows the distribution of predicted probabilities for true positives and true negatives for the interaction-enhanced model. Correctly classified positives cluster near high scores, and correctly classified negatives cluster near low scores, which indicates that the model meaningfully separates the classes. Most misclassified examples lie in the ambiguous middle region around 0.4 to 0.6, which suggests that the model is mainly struggling on intrinsically hard or noisy cases rather than producing many highly confident errors.

\subsection{False Positives vs. False Negatives}

Qualitative inspection of individual pairings helps us to interpret the types of mistakes the model makes.

\paragraph{False positives.} False positives often arise when participant A and B look highly compatible in terms of observable survey variables but A nevertheless chose ``no'' during the event. Typical patterns include:
\begin{itemize}
    \item B matches A's stated ideal partner profile (for example similar age, similar race, and high self ratings on attractiveness and fun),
    \item A reports strong interest in activities that B also rates highly (for example music and movies), and
    \item the interaction deltas (\texttt{d\_age}, \texttt{d\_attractive}, \texttt{d\_funny}, \texttt{d\_music}) are small.
\end{itemize}
In such cases the model assigns a high probability of a positive decision, yet the ground truth label is negative. This suggests that some errors are driven by factors that are not captured in the pre-event survey such as physical chemistry, mood, or subtle conversational dynamics. From a recommendation perspective these false positives are often still plausible matches, but they hurt measured precision.

\paragraph{False negatives.} False negatives frequently appear in situations where the pre-event features suggest only moderate compatibility:
\begin{itemize}
    \item A and B differ substantially on one or more self ratings (for example A rates themselves highly on ambition while B does not),
    \item there are noticeable gaps in interests (for example A reports high interest in sports while B reports low sports interest), or
    \item A's stated partner preferences emphasize traits that B only weakly exhibits.
\end{itemize}
Despite these apparent mismatches, the ground truth label is positive. Here, the model tends to predict ``no'' because it treats these misalignments as evidence against compatibility. This behavior indicates that the model has learned strong priors about trait alignment and may underweight idiosyncratic cases where attraction occurs despite measurable differences.

\subsection{Feature-Dependent Error Patterns}

We next analyzied how error rates vary as a function of key feature groups.

\paragraph{Preference gap and interaction deltas.} The addition of interaction features (\texttt{d\_*}) substantially reduces errors where A and B are similar on absolute traits but differ in relative preferences. The base XGBoost model, which only sees separate A and B features, often misclassifies pairs where both individuals have generally high self ratings but value different traits in partners. The interaction-enhanced model corrects many of these mistakes by explicitly modeling gaps such as cases where A strongly values sincerity while B places more emphasis on fun. Remaining errors in this category typically involve medium-sized deltas where the model is uncertain.

\paragraph{Participant heterogeneity.} Some participants are more difficult to model than others. Participants who say ``yes'' extremely rarely or extremely often create skewed local label distributions. For example, very selective participants generate many negative labels even for objectively strong candidates, which leads the model to overpredict ``yes'' in rare instances when they do accept a partner. On the other hand, participants who say ``yes'' to almost everyone cause the model to underpredict ``no'' for the few pairings they reject. This effect is partially mitigated by splitting data at the participant level and by including individual-level preferences, but it does not fully go away.

\paragraph{Demographic subgroups.} When grouping errors by demographic attributes, we observe that performance is strongest in well-represented subgroups such as younger participants in the most common racial categories and fields of study. Error rates are higher for older participants and under-represented demographic groups, where the model has fewer examples from which to learn stable patterns. This is an expected limitation given the dataset size and distribution and suggests that some predictions may be less reliable for small subpopulations.

\subsection{Comparison Across Models}

Comparing error patterns across models highlights how modeling choices affect behavior. Figure~\ref{fig:metric-bars-base-vs-rich} summarizes their test-set metrics.

\begin{itemize}
    \item The majority baseline achieves high accuracy on the negative class but has zero recall on positive decisions. All its errors are false negatives, so it is unusable as a recommender.
    \item The calibrated XGBoost model trained on the base feature set improves substantially over the majority classifier but still treats many ambiguous cases as negative. Without interaction features it struggles when compatibility depends on relative differences between A and B rather than absolute traits alone.
    \item The interaction-enhanced XGBoost model reduces both false positives and false negatives relative to these baselines, with the largest gains on pairs where compatibility is driven by nuanced tradeoffs between attractiveness, fun, sincerity, and shared interests. As shown in Figure~\ref{fig:roc-calib-base-vs-rich}, it also dominates the base model in ROC space and has better-calibrated probabilities.
\end{itemize}

\subsection{Opportunities for Improvement}

The observed error patterns suggest several concrete extensions if we were to continue this work:
\begin{itemize}
    \item \textbf{Cost-sensitive training.} Assigning a higher loss weight to positive examples could reduce false negatives at the cost of tolerating more false positives. This may be acceptable for a recommender that aims to avoid missing promising matches.
    \item \textbf{Participant-specific calibration.} Calibrating probabilities separately for each participant, or adding explicit per-participant bias terms, could better account for individual differences in selectivity and reduce systematic errors for very selective or very permissive users.
\end{itemize}

Overall, the error analysis indicates that the current model captures many regularities in pre-event compatibility but remains limited by label noise, participant heterogeneity, and sparse coverage for minority subgroups. The proposed extensions target these specific weaknesses and provide a roadmap for increasing both predictive performance and fairness in future iterations.




\section{Progress Reflection}

Our final system remained closely aligned with the goals and methodology outlined
in our original progress report, while also evolving in several important ways as
our understanding of the dataset and modeling challenges deepened. Below, we
reflect on the areas where we followed the planned trajectory and where we
deviated to improve performance or feasibility. These comparisons are
based on the commitments documented in our earlier progress report
as well as the project design described in our project outline.

\subsection{Adherence to the Original Plan}

\paragraph{Leakage-free pre-event modeling.}
From the start, we committed to using only pre-event survey features to avoid
any contamination from post-date impressions. Our final models strictly adhered
to this constraint, and all feature sets including the rich interaction
features were constructed using only information available prior to the speed
date. This followed the design principles of our initial report, which emphasized
fairness, reproducibility, and the avoidance of post-event leakage.

\paragraph{Focus on A$\rightarrow$B decision prediction.}
The progress report identified the A$\rightarrow$B decision classifier as the
foundation for the entire pipeline, and our final system delivered a fully
optimized version of this model. We completed the full evaluation suite we had
outlined, including accuracy, precision, recall, F1, and ROC-AUC, fulfilling our
original modeling goals.

\paragraph{Use of gradient-boosted decision trees.}
We initially proposed XGBoost as our primary model because of its ability to
capture nonlinear interactions in heterogeneous data. Our final implementation
remained faithful to this choice and expanded upon it with calibration through
isotonic regression, as originally planned. We also implemented the simpler HGB
baseline exactly as described in the earlier report.

\paragraph{Integration of interaction (delta) features.}
The earlier report anticipated adding interaction features capturing pairwise
differences (e.g., age gap, rating gaps, shared traits). In the final work we not
only implemented these but expanded them substantially, incorporating all
non-leaky \texttt{d\_*} features present in the dataset. This expansion proved to
be the key driver of improved predictive performance, validating our initial
hypothesis that compatibility is fundamentally relational.

\subsection{Where We Deviated From or Expanded Upon the Original Plan}

\paragraph{Model variety and planned alternatives.}
The progress report discussed experimenting with logistic regression, decision
trees, LightGBM, and CatBoost. In practice, we discovered that implementing and
optimizing multiple additional models provided diminishing returns relative to
investing more effort into XGBoost feature engineering and calibration.
Therefore, we focused on two strong baselines (majority-class and HGB) and two
XGBoost-based systems (base and interaction-enhanced), deviating from the
original expectation of a broader model comparison.


% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \subsection{Citations}


% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Eastwick:2014}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Roth:online}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Eastwick:2014}).

% \subsection{References}

% Many websites where you can find academic papers also allow you to export a bib file for citation or bib formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the \LaTeX{}. You can remove the example entries.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.
% This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like this: $A=\pi r^2$.


% \section*{Limitations}

\section*{Team Contributions}

Om led the model development and implementation process, including data preprocessing, feature engineering, and training the XGBoost classifier. He also integrated the calibration and evaluation framework and coordinated the A~$\rightarrow$~B~$\rightarrow$~C pipeline design.

Gregory focused on exploratory data analysis, dataset cleaning, and the construction of participant-level profiles. He was responsible for identifying key features from the survey data and preparing visualizations and summary statistics used in both the report and presentation materials.

Alvin contributed to the experimental design and evaluation setup, helping define baseline comparisons and select appropriate performance metrics. He also supported documentation, result interpretation, and report writing, ensuring clarity and consistency across all sections.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

\clearpage
\onecolumn
\section*{Tables and Figures}

% Table~\ref{tab:top10-recommendations} shows the top 10 recommended partners for a sample participant A, ranked by predicted compatibility score (p\_AB). The table illustrates the model's ability to identify high-potential matches based on pre-event survey data.
\begin{figure}[H]
  \raggedright
  \includegraphics[width= \textwidth]{plots/true_false_positives.png}
  \caption{Distribution of predicted probabilities by actual outcome, showing the model's ability to discriminate between true positives and true negatives.}
  \label{fig:true-false-positives}
\end{figure}

\begin{table}[H]
\raggedright
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}cccccccccccccccc@{}}
\hline
\textbf{ID} & \textbf{Gen.} & \textbf{Age} & \textbf{Race} & \textbf{Field} & \textbf{Att.} & \textbf{Sin.} & \textbf{Int.} & \textbf{Fun.} & \textbf{Amb.} & \textbf{Spo.} & \textbf{Mus.} & \textbf{Mov.} & \textbf{Rea.} & \textbf{Exe.} & \textbf{p\_AB} \\
\hline
137 & F & 29 & Oth. & Music Ed. & 9 & 10 & 9 & 9 & 8 & 8 & 10 & 9 & 5 & 10 & 0.64 \\
439 & F & 27 & Eur. & Finance & 8 & 10 & 10 & 9 & 10 & 7 & 10 & 10 & 5 & 10 & 0.64 \\
367 & F & 26 & Lat. & Law & 9 & 9 & 9 & 9 & 9 & 8 & 9 & 9 & 7 & 7 & 0.64 \\
199 & F & 29 & Eur. & Psychology & 7 & 8 & 4 & 8 & 8 & 6 & 6 & 7 & 7 & 4 & 0.64 \\
198 & F & 28 & Eur. & Social Work & 9 & 8 & 5 & 9 & 3 & 6 & 6 & 9 & 9 & 5 & 0.64 \\
369 & F & 28 & Eur. & German Lit. & 7 & 10 & 7 & 10 & 7 & 1 & 10 & 10 & 10 & 5 & 0.64 \\
370 & F & 29 & Oth. & Psychology & 7 & 9 & 9 & 9 & 9 & 3 & 9 & 7 & 5 & 9 & 0.64 \\
194 & F & 22 & Eur. & Social Work & 8 & 9 & 7 & 10 & 7 & 8 & 5 & 7 & 9 & 5 & 0.64 \\
382 & F & 22 & Eur. & Comm. & 7 & 9 & 9 & 9 & 4 & 2 & 10 & 10 & 4 & 1 & 0.64 \\
383 & F & 22 & Eur. & Social Work & 6 & 8 & 8 & 8 & 8 & 7 & 10 & 8 & 6 & 10 & 0.64 \\
\hline
\end{tabular}
\caption{Top 10 recommended partners for sample participant A.}
\label{tab:top10-recommendations}
\end{table}

% Table~\ref{tab:composite-profile} presents the composite profile B*, constructed by averaging the characteristics of the top 10 recommended partners shown in Table~\ref{tab:top10-recommendations}. This aggregated profile represents the ``ideal partner type'' that participant A would most likely prefer, based on the model's predictions.

\begin{table}[H]
\raggedright
\begin{tabular}{@{}lr@{}}
\hline
\textbf{Attribute} & \textbf{Value} \\
\hline
Age & 26.2 \\
Attractive & 7.7 \\
Sincere & 9.0 \\
Intelligence & 7.7 \\
Funny & 9.0 \\
Ambition & 7.3 \\
Sports & 5.6 \\
Music & 8.5 \\
Movies & 8.6 \\
Reading & 6.7 \\
Exercise & 6.6 \\
Gender & Female \\
Race & European/Caucasian-American \\
Field & Social Work \\
\hline
\end{tabular}
\caption{Composite profile B* derived by averaging the top 10 recommended partners for participant A. Numeric values represent means across all candidates; categorical values represent the mode (most common value).}
\label{tab:composite-profile}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{plots/updated_graphs.png}
  \caption{Comparison of ROC (left) and calibration (right) curves for calibrated XGBoost models using the base feature set and the base plus interaction feature set. The interaction-enhanced model achieves a higher ROC-AUC and a reliability curve closer to the diagonal.}
  \label{fig:roc-calib-base-vs-rich}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{plots/comparison_bar_chart.png}
  \caption{Test-set metric comparison for the base and interaction-enhanced XGBoost models. Bars show accuracy, precision, recall, F1, and ROC-AUC, all of which improve when interaction features are included.}
  \label{fig:metric-bars-base-vs-rich}
\end{figure}


% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
